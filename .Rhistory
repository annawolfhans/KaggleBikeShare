mean(diffs=0)
mean(diffs<0)
#98
quantile(diffs, c(0.01,.99))
diffs <- (beta1_h[keep]-beta1[keep])
mean(diffs)
#95% credible interval
quantile(diffs, c(0.025, 0.975))
#98% credible interval
quantile(diffs, c(0.01,.99))
acf(t.post[,1])[1:10] # mu testing, excellent mixing ( I can tell because the lag isn't mixing more slowly)
acf(t.post[,2])[1:10] # sigma2 testing, excellent mixing
acf(c.post[,1])[1:10] # mu control, excellent mixing
acf(c.post[,2])[1:10] # sigma2 control, excellent mixing
acf(t.post[,1]) # mu testing, excellent mixing ( I can tell because the lag isn't mixing more slowly)
acf(t.post[,2])# sigma2 testing, excellent mixing
acf(c.post[,1]) # mu control, excellent mixing
acf(c.post[,2]) # sigma2 control, excellent mixing
t.post[,1]
acf(t.post[keep,1]) # mu testing, excellent mixing ( I can tell because the lag isn't mixing more slowly)
acf(t.post[keep,2])# sigma2 testing, excellent mixing
acf(c.post[keep,1]) # mu control, excellent mixing
acf(c.post[keep,2]) # sigma2 control, excellent mixing
par(mfrow=c(2,2))
acf(t.post[keep,1]) # mu testing, excellent mixing ( I can tell because the lag isn't mixing more slowly)
acf(t.post[keep,2])# sigma2 testing, excellent mixing
acf(c.post[keep,1]) # mu control, excellent mixing
acf(c.post[keep,2]) # sigma2 control, excellent mixing
keep <- seq(1000, J, by=100)
par(mfrow=c(3,2))
plot(beta0_h[keep], type='l')
plot(beta1_h[keep], type='l')
plot(sigma2_h[keep], type='l')
par(mfrow=c(3,2))
acf(beta0_h[keep])
acf(beta1_h[keep])
acf(sigma2_h[keep])
keep <- seq(1, J, by=10)
keep <- seq(1, J, by=10)+10
acf(beta0_h[keep])
beta0_h <- beta1_h <- sigma2_h <- numeric()
lm1_h <- lm(bp_h ~ bmi_h)
beta0_h[1] <- lm1_h$coefficients[1]
beta1_h[1] <- lm1_h$coefficients[2]
sigma2_h[1] <- summary(lm1_h)$sigma^2
# The number of MCMC iterations
J <- 100000
n_h <- length(bp_h)
for(j in 2:J){
# update beta0 by sampling from its complete conditional
m0star_h <- ((1/sigma2_h[j-1])*sum(bp_h - beta1_h[j-1]*bmi_h) + (1/v0)*m0)/
(n_h/sigma2_h[j-1] + 1/v0)
v0star_h <- 1/(n_h/sigma2_h[j-1] + 1/v0)
beta0_h[j] <- rnorm(1, m0star_h, sqrt(v0star_h))
# update beta1 by sampling from its complete conditional
m1star_h <- ((1/sigma2_h[j-1])*sum(bmi_h*(bp_h - beta0_h[j])) + (1/v1)*m1)/
(sum(bmi_h^2)/sigma2_h[j-1] + 1/v1)
v1star_h <- 1/(sum(bmi_h^2)/sigma2_h[j-1] + 1/v1)
beta1_h[j] <- rnorm(1, m1star_h, sqrt(v1star_h))
# update sigma2 by sampling from its complete conditional using a metropolis step
sig2_cand_h <- rnorm(1, sigma2_h[j-1], 50)
sigma2_h[j] <- sigma2_h[j-1]
if(sig2_cand_h > 0){
lpcand_h <- sum(dnorm(bp_h, beta0_h[j] + beta1_h[j]*bmi_h, sqrt(sig2_cand_h), log=TRUE)) +
dinvgamma(sig2_cand_h, a, b, log=TRUE)
lpcurr_h <- sum(dnorm(bp_h, beta0_h[j] + beta1_h[j]*bmi_h, sqrt(sigma2_h[j-1]), log=TRUE)) +
dinvgamma(sigma2_h[j-1], a, b, log=TRUE)
alpha_h <- min(1, exp(lpcand_h - lpcurr_h))
u <- runif(1,0,1)
if(alpha_h > u) sigma2_h[j] <- sig2_cand_h
}
}
keep <- seq(1, J, by=10)+10
mean(diff(sigma2_h[keep]) != 0)
# check for convergence
par(mfrow=c(3,2))
plot(beta0_h[keep], type='l')
plot(beta1_h[keep], type='l')
plot(sigma2_h[keep], type='l')
#Similar to the previous plots above, given the speedy convergence here (as opposed to more spread, clear lines), all three of these are meeting what I'dlike for convergence
# check for mixing
par(mfrow=c(3,2))
acf(beta0_h[keep])
keep <- seq(10, J, by=100)
acf(beta0_h[keep])
acf(beta1_h[keep])
acf(sigma2_h[keep])
keep <- seq(10, J, by=50)
par(mfrow=c(3,2))
acf(beta0_h[keep])
acf(beta1_h[keep])
acf(sigma2_h[keep])
acf(beta0_h[keep])
acf(beta1_h[keep])
acf(sigma2_h[keep])
par(mfrow=c(3,1))
acf(beta0_h[keep])
acf(beta1_h[keep])
acf(sigma2_h[keep])
par(mfrow=c(1,3))
acf(beta0_h[keep])
acf(beta1_h[keep])
acf(sigma2_h[keep])
par(mfrow=c(1,3))
acf(beta0[keep])
acf(beta1[keep])
acf(sigma2[keep])
par(mfrow=c(4,1))
acf(t.post[keep,1]) # mu testing, excellent mixing ( I can tell because the lag isn't mixing more slowly)
par(mfrow=c(1,4))
acf(t.post[keep,1]) # mu testing, excellent mixing ( I can tell because the lag isn't mixing more slowly)
acf(t.post[keep,2])# sigma2 testing, excellent mixing
acf(c.post[keep,1]) # mu control, excellent mixing
acf(c.post[keep,2]) # sigma2 control, excellent mixing
usethis::create_github_token(PAT)
usethis::create_github_token()
usethis::create_github_token()
foo <- function(
x,
y,
z){
w <- x+y-z
return(w)
}
foo(3,5,6)
f1(disappear, dis)
f1 <- function(string, prefix) {
substr(string, 1, nchar(prefix)) == prefix
}
f1(disappear, dis)
f1(c(disappear))
f2 <- function(x) {
if (length(x) <= 1) return(NULL)
x[-length(x)]
}
f3 <- function(x, y) {
rep(y, length.out = length(x))
}
f2(2134)
f2("2134")
f2("disappear")
f1("disappear")
devtools::install_github("jntrcs/BnpSysRel")
library(BnpSysRel)
text <- "S(BackTire, FrontTire):Tires
P(BackBrake, FrontBrake): Brakes
S(Tires, Brakes): Bike"
file <- textConnection(text)
times <- seq(10,100,length.out= 10)
tirePrior <- bsp(support = times, centeringMeasure = pnorm(times, 50, 15),
precision = 3)
plot(tirePrior, withConfInt = T)
plot(tirePrior, withConfInt = F)
plot(tirePrior, withConfInt = T)
brakePrior <- bsp(support = c(22,44,66), centeringMeasure = c(0.25,0.5,0.75),
precision = 1)
priorList <- list(FrontTire = tirePrior, BackTire = tirePrior,
FrontBrake = brakePrior, BackBrake= brakePrior)
backTireData <- matrix(c(14,1,
29,1
67,1,
backTireData <- matrix(c(14,1,
29,1,
67,1,
75,0,
75,0), byrow = T, nrow = 5)
set.seed(20)
library(BnpSysRel)
text <- "S(BackTire, FrontTire):Tires
P(BackBrake, FrontBrake): Brakes
S(Tires, Brakes): Bike"
file <- textConnection(text)
times <- seq(10,100,length.out = 10)
# the bsp() function creates betaStacyProcess objects (we do this to keep conjugacy)
tirePrior <- bsp(support = times, centeringMeasure = pnorm(times, 50, 15),
precision = 3)
plot(tirePrior, withConfInt = T)
?plot
# the bsp() function creates betaStacyProcess objects (we do this to keep conjugacy)
# basically defining what are best guess is to what the cdf of the component should be
tirePrior <- bsp(support = times, centeringMeasure = pnorm(times, 50, 15),
precision = 30)
# support is the list of jumps we want to see in our prior
# centeringMeasure is how large those jumps should be
# precision is measure of how certain we are in that cdf
plot(tirePrior, withConfInt = T)
brakePrior <- bsp(support = c(22,44,66), centeringMeasure = c(0.25,0.5,0.75),
precision = 1)
#priorList will specify priors we want to use on the components
priorList <- list(FrontTire = tirePrior, BackTire = tirePrior,
FrontBrake = brakePrior, BackBrake= brakePrior)
backTireData <- matrix(c(14,1,
29,1,
67,1,
75,0,
75,0), byrow = T, nrow = 5)
set.seed(20)
frontBrake <- cbind(rchisq(40,20), 1)
frontTireData <- cbind(rnorm(50,50,10), 1)
backBrake <- cbind(rnorm(30, 20, 5), 1)
bike <- cbind(c(40,47,42,46), 1)
dataList <- list(BackTire = backTireData,
FrontTire=frontTireData,
BackBrake=backBrake,
FrontBrake=frontBrake,
Bike=bike)
posteriors = estimateSystemReliability(file=file,
priorList=priorList,
dataList=dataList)
# ANALYSIS
plot(posteriors$Bike, withConfInt = T)
#now we can get a point estimate and cred interval on % of bikes
# that have failed by t=20 and t=50 months
evaluate_centering_measure(posteriors$Bike, times=c(20,50))
#now we can get a point estimate and cred interval on % of bikes
# that have failed by t=20 and t=50 months
evaluate_centering_measure(posteriors$Bike, times=c(20,21,50))
bspConfint(posteriors$Bike, times = c(20,50), cred.level=0.95)
bspConfint(posteriors$Bike, times = c(20,50), cred.level = .95)
bspConfint(posteriors$Bike, times = c(20,50), cred.level = .95)
#now we can get a point estimate and cred interval on % of bikes
# that have failed by t=20 and t=50 months
evaluate_centering_measure(posteriors$Bike, times=c(20,50))
bspConfint(posteriors$Bike, times = c(20,50), cred.level = .95)
bspConfint(posteriors$Bike, times = c(20,50))
quantile(posteriors$Bike, probs=c(0.1,0.9))
bspConfint(posteriors$Bike, times = c(20,50))
quantile(posteriors$Bike, probs=c(0.1,0.9))
# IF YOU ONLY HAVE A SINGLE COMPONENT
BackTirePosterior <- bspPosterior(priorList$BackTire, dataList$BackTire)
t1 <- print(posteriors$BackTire)
t2 <- print(BackTirePosterior)
knitr::kable(t1)
knitr::kable(t2)
newfunct <- function (x, graph = FALSE) {
x <- x+1
if (graph) {
plot(x)
}
}
newfunct(3)
newfunct <- function (x, graph = FALSE) {
x <- x+1
if (graph) {
plot(x)
}
print(x)
}
newfunct(3)
newfunct(3, graph = TRUE)
data <- "A             B                C
1  2.672032 3.94838375 3.259812
2  4.992845 4.60384630 1.873793
3  3.850774 0.71413085 2.255734
4  3.349399 1.06260383 1.896054
5  3.693392 0.36243553 4.752043
6  3.351725 1.06418089 3.763228
7  3.054851 1.76343783 3.464489
8  2.337146 1.91491107 1.741845
9  3.799727 0.22896404 4.055936
10 3.423747 0.47780975 3.642198
11 2.147855 0.70410380 3.230790
12 2.914339 2.07657045 3.610964
13 2.822960 1.96488912 3.647586
14 1.487045 1.00575861 1.416564
15 3.507597 0.05270144 1.588945
16 2.035947 0.65919314 2.171989
17 1.984040 0.04479124 3.137354
18 4.428933 0.84922901 3.914643
19 5.054555 1.42763691 2.985993
20 1.521784 1.85469872 2.343341
21 1.735590 0.13412839 3.565964
22 4.822847 1.20728311 4.097896
23 4.005707 2.14166822 2.368822
24 3.538016 0.17176038 3.984387
25 1.328386 0.46659910 3.734879
26 3.134757 0.08238957 2.189424
27 3.169516 0.44278178 3.354351
28 4.139189 0.26110072 5.524083
29 4.826068 0.28843847 2.219326
30 3.773776 3.03941043 3.987104"
some_data <- read.csv("some_data.csv")
Some_data <- read.table("~/STAT435/Some_data.txt", header=TRUE, quote="\"")
View(Some_data)
# perform Import dataset
Some_data
B_data <- Some_data %>%
select("B")
B_data <- Some_data$B
(B_data <- Some_data$B)
class(B_data)
summary(B_data)
ggplot(B_data)
library(ggfortify)
ggplot(B_data)
ggplot(Some_data)
summary(B_data)
plot(summary(B_data))
plot(summary(Some_data))
plot(Some_data$B, Some_data$A)
# Note that mean does not equal the median
# Skewed right
var(B_data)
Some_data <- read_csv("STAT435/some_data.csv")
# perform Import dataset
library(readr)
Some_data <- read_csv("STAT435/some_data.csv")
Some_data <- read_csv("STAT435/some_data.csv", sep = " ")
Some_data <- read_csv("STAT435/some_data.csv", col_names = T)
Some_data
Some_data$B
Some_data <- read.table("~/STAT435/Some_data.txt", header=TRUE, quote="\"")
View(Some_data)
# perform Import dataset
Some_data$A
# perform Import dataset
Some_data$B
# perform Import dataset
Some_data$C
Some_data <- data.frame(
A = c(2.672032, 4.992845, 3.850774, 3.349399, 3.693392, 3.351725, 3.054851, 2.337146,
3.799727, 3.423747, 2.147855, 2.914339, 2.822960, 1.487045, 3.507597 ,2.035947,
1.984040, 4.428933, 5.054555, 1.521784, 1.735590, 4.822847, 4.005707, 3.538016
1.328386 ,3.134757 ,3.169516, 4.139189, 4.826068, 3.773776),
Some_data <- data.frame(
A = c(2.672032, 4.992845, 3.850774, 3.349399, 3.693392, 3.351725, 3.054851, 2.337146,
3.799727, 3.423747, 2.147855, 2.914339, 2.822960, 1.487045, 3.507597 ,2.035947,
1.984040, 4.428933, 5.054555, 1.521784, 1.735590, 4.822847, 4.005707, 3.538016,
1.328386 ,3.134757 ,3.169516, 4.139189, 4.826068, 3.773776),
B = c(3.94838375, 4.60384630, 0.71413085, 1.06260383 ,0.36243553 ,1.06418089,1.76343783 ,1.91491107, 0.22896404,
0.47780975, 0.70410380, 2.07657045, 1.96488912, 1.00575861, 0.05270144, 0.65919314, 0.04479124, 0.84922901,
1.42763691, 1.85469872, 0.13412839, 1.20728311, 2.14166822, 0.17176038, 0.46659910, 0.08238957, 0.44278178,
0.26110072, 0.28843847, 3.03941043)
C = c(3.259812, 1.873793, 2.255734, 1.896054, 4.752043, 3.763228, 3.464489, 1.741845, 4.055936, 3.642198, 3.230790, 3.610964, 3.647586, 1.416564,
Some_data <- data.frame(
A = c(2.672032, 4.992845, 3.850774, 3.349399, 3.693392, 3.351725, 3.054851, 2.337146,
3.799727, 3.423747, 2.147855, 2.914339, 2.822960, 1.487045, 3.507597 ,2.035947,
1.984040, 4.428933, 5.054555, 1.521784, 1.735590, 4.822847, 4.005707, 3.538016,
1.328386 ,3.134757 ,3.169516, 4.139189, 4.826068, 3.773776),
B = c(3.94838375, 4.60384630, 0.71413085, 1.06260383 ,0.36243553 ,1.06418089,1.76343783 ,1.91491107, 0.22896404,
0.47780975, 0.70410380, 2.07657045, 1.96488912, 1.00575861, 0.05270144, 0.65919314, 0.04479124, 0.84922901,
1.42763691, 1.85469872, 0.13412839, 1.20728311, 2.14166822, 0.17176038, 0.46659910, 0.08238957, 0.44278178,
0.26110072, 0.28843847, 3.03941043),
C = c(3.259812, 1.873793, 2.255734, 1.896054, 4.752043, 3.763228, 3.464489, 1.741845, 4.055936, 3.642198, 3.230790, 3.610964, 3.647586, 1.416564,
1.588945, 2.171989, 3.137354 ,3.914643 ,2.985993 ,2.343341 ,3.565964, 4.097896 ,2.368822, 3.984387, 3.734879, 2.189424, 3.354351 ,5.524083,
2.219326, 3.987104)
)
Some_data
Some_data$A
library(ggplot2)
ggplot(Some_data, aes(y = Values)) +
geom_boxplot() +
ggtitle("Box and Whisker Plot") +
ylab("Values")
ggplot(Some_data, aes(y)) +
geom_boxplot() +
ggtitle("Box and Whisker Plot") +
ylab("Values")
View(Some_data)
ggplot(Some_data) +
geom_boxplot() +
ggtitle("Box and Whisker Plot")
boxplot(Some_data)
t.test(Some_data$B, Some_data$C)
t.test(Some_data$B, Some_data$A)
t.test(Some_data$B, Some_data$B)
t.test(Some_data$B, Some_data$A)
setwd("~/STAT348/KaggleBikeShare")
## Libraries I am going to need
library(tidyverse)
library(tidymodels)
library(vroom)
library(rpart)
## Read in the data
bikeTrain <- vroom("./train.csv")
bikeTest <- vroom("./test.csv")
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)%>%
mutate(count=log(count))
## Cleaning & Feature Engineering
bike_preg_recipe <- recipe(count~., data=bikeTrain) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
# step_mutate(year=factor(year)) %>%
step_time(datetime, features="hour") %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_preg_recipe <- prep(bike_preg_recipe)
## Define the model using boost_trees
forest_model <- boost_tree(mtry=tune(),
min_n=tune(),
trees=500) %>%
set_engine("xgboost") %>%
set_mode("regression")
## Set up the whole workflow
forest_wf <- workflow() %>%
add_recipe(bike_preg_recipe) %>%
add_model(forest_model)
tuning_grid <- grid_regular(mtry(range =c(1,10)),
min_n(),
levels = 5)
folds <- vfold_cv(bikeTrain, v = 5, repeats = 2)
forest_results <- forest_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
install.packages("xgboost")
forest_results <- forest_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
bestTune <- forest_results %>%
select_best("rmse")
final_wf <- forest_wf %>%
finalize_workflow(bestTune) %>%
fit(data = bikeTrain)
final_wf %>%
predict(new_data = bikeTest)
## Get Predictions for test set AND format for Kaggle
test_forests_preds <- predict(final_wf, new_data = bikeTest) %>%
mutate(.pred=exp(.pred)) %>%
bind_cols(., bikeTest) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
# run downs rows and if 0 is bigger, it will replace it with 0, otherwise use count
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
# vroom will add rando variables unless you make it character string
vroom_write(x=test_forests_preds, file="./TestPredsForest.csv", delim=",")
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)%>%
mutate(count=log(count))
## Cleaning & Feature Engineering
bike_preg_recipe <- recipe(count~., data=bikeTrain) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_time(datetime, features="hour, year") %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)%>%
mutate(count=log(count))
## Read in the data
bikeTrain <- vroom("./train.csv")
bikeTest <- vroom("./test.csv")
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)%>%
mutate(count=log(count))
## Cleaning & Feature Engineering
bike_preg_recipe <- recipe(count~., data=bikeTrain) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_time(datetime, features="hour, year") %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Cleaning & Feature Engineering
bike_preg_recipe <- recipe(count~., data=bikeTrain) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_date(datetime, features="hour", "year") %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
## Read in the data
bikeTrain <- vroom("./train.csv")
bikeTest <- vroom("./test.csv")
## Remove casual and registered because we can't use them to predict
bikeTrain <- bikeTrain %>%
select(-casual, - registered)%>%
mutate(count=log(count))
bikeTrain$year <- lubridate::year(bikeTrain$datetime)
## Cleaning & Feature Engineering
bike_preg_recipe <- recipe(count~., data=bikeTrain) %>%
step_mutate(weather=ifelse(weather==4, 3, weather)) %>% #Relabel weather 4 to 3
step_mutate(weather=factor(weather, levels=1:3, labels=c("Sunny", "Mist", "Rain"))) %>%
step_time(datetime, features="hour") %>%
step_rm(datetime) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_preg_recipe <- prep(bike_preg_recipe)
## Define the model
forest_model <- rand_forest(mtry = tune(),
min_n=tune(),
trees=500) %>%
set_engine("ranger") %>%
set_mode("regression")
## Set up the whole workflow
forest_wf <- workflow() %>%
add_recipe(bike_preg_recipe) %>%
add_model(forest_model)
tuning_grid <- grid_regular(mtry(range =c(1,10)),
min_n(),
levels = 5)
folds <- vfold_cv(bikeTrain, v = 5, repeats = 2)
forest_results <- forest_wf %>%
tune_grid(resamples=folds,
grid=tuning_grid,
metrics=metric_set(rmse, mae, rsq))
bestTune <- forest_results %>%
select_best("rmse")
final_wf <- forest_wf %>%
finalize_workflow(bestTune) %>%
fit(data = bikeTrain)
final_wf %>%
predict(new_data = bikeTest)
